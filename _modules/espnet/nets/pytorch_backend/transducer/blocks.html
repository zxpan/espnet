

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>espnet.nets.pytorch_backend.transducer.blocks &mdash; ESPnet 0.9.8 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../../../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../../index.html" class="icon icon-home"> ESPnet
          

          
          </a>

          
            
            
              <div class="version">
                0.9.8
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Tutorial:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorial.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../parallelization.html">Using Job scheduling system</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../docker.html">Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">ESPnet2:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../espnet2_tutorial.html">ESPnet2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../espnet2_tutorial.html#instruction-for-run-sh">Instruction for run.sh</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../espnet2_training_option.html">Change the configuration for training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../espnet2_task.html">Task class and data input system for training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../espnet2_distributed.html">Distributed training</a></li>
</ul>
<p class="caption"><span class="caption-text">Notebook:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notebook/asr_cli.html">Speech Recognition (Recipe)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notebook/asr_library.html">Speech Recognition (Library)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notebook/tts_cli.html">Text-to-Speech (Recipe)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notebook/pretrained.html">Pretrained Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notebook/tts_realtime_demo.html">ESPnet real time E2E-TTS demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notebook/st_demo.html">ESPnet Speech Translation Demonstration</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet.st.html">espnet.st package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet.lm.html">espnet.lm package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet.tts.html">espnet.tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet.transform.html">espnet.transform package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet.bin.html">espnet.bin package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet.scheduler.html">espnet.scheduler package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet.vc.html">espnet.vc package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet.utils.html">espnet.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet.optimizer.html">espnet.optimizer package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet.nets.html">espnet.nets package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet.mt.html">espnet.mt package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet.asr.html">espnet.asr package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet2.tasks.html">espnet2.tasks package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet2.text.html">espnet2.text package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet2.optimizers.html">espnet2.optimizers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet2.torch_utils.html">espnet2.torch_utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet2.iterators.html">espnet2.iterators package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet2.lm.html">espnet2.lm package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet2.tts.html">espnet2.tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet2.bin.html">espnet2.bin package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet2.layers.html">espnet2.layers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet2.schedulers.html">espnet2.schedulers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet2.enh.html">espnet2.enh package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet2.utils.html">espnet2.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet2.main_funcs.html">espnet2.main_funcs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet2.asr.html">espnet2.asr package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet2.fileio.html">espnet2.fileio package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet2.samplers.html">espnet2.samplers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../_gen/espnet2.train.html">espnet2.train package</a></li>
</ul>
<p class="caption"><span class="caption-text">Tool Reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../apis/espnet_bin.html">core tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../apis/espnet2_bin.html">core tools (espnet2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../apis/utils_py.html">python utility tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../apis/utils_sh.html">bash utility tools</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">ESPnet</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../../index.html">Module code</a> &raquo;</li>
        
      <li>espnet.nets.pytorch_backend.transducer.blocks</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for espnet.nets.pytorch_backend.transducer.blocks</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Set of methods to create custom architecture.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">espnet.nets.pytorch_backend.conformer.convolution</span> <span class="kn">import</span> <span class="n">ConvolutionModule</span>
<span class="kn">from</span> <span class="nn">espnet.nets.pytorch_backend.conformer.encoder_layer</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">EncoderLayer</span> <span class="k">as</span> <span class="n">ConformerEncoderLayer</span><span class="p">,</span>  <span class="c1"># noqa: H301</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">espnet.nets.pytorch_backend.nets_utils</span> <span class="kn">import</span> <span class="n">get_activation</span>

<span class="kn">from</span> <span class="nn">espnet.nets.pytorch_backend.transducer.causal_conv1d</span> <span class="kn">import</span> <span class="n">CausalConv1d</span>
<span class="kn">from</span> <span class="nn">espnet.nets.pytorch_backend.transducer.transformer_decoder_layer</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">DecoderLayer</span><span class="p">,</span>  <span class="c1"># noqa: H301</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">espnet.nets.pytorch_backend.transducer.tdnn</span> <span class="kn">import</span> <span class="n">TDNN</span>
<span class="kn">from</span> <span class="nn">espnet.nets.pytorch_backend.transducer.vgg2l</span> <span class="kn">import</span> <span class="n">VGG2L</span>

<span class="kn">from</span> <span class="nn">espnet.nets.pytorch_backend.transformer.attention</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">MultiHeadedAttention</span><span class="p">,</span>  <span class="c1"># noqa: H301</span>
    <span class="n">RelPositionMultiHeadedAttention</span><span class="p">,</span>  <span class="c1"># noqa: H301</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">espnet.nets.pytorch_backend.transformer.encoder_layer</span> <span class="kn">import</span> <span class="n">EncoderLayer</span>
<span class="kn">from</span> <span class="nn">espnet.nets.pytorch_backend.transformer.embedding</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">PositionalEncoding</span><span class="p">,</span>  <span class="c1"># noqa: H301</span>
    <span class="n">ScaledPositionalEncoding</span><span class="p">,</span>  <span class="c1"># noqa: H301</span>
    <span class="n">RelPositionalEncoding</span><span class="p">,</span>  <span class="c1"># noqa: H301</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">espnet.nets.pytorch_backend.transformer.positionwise_feed_forward</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">PositionwiseFeedForward</span><span class="p">,</span>  <span class="c1"># noqa: H301</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">espnet.nets.pytorch_backend.transformer.repeat</span> <span class="kn">import</span> <span class="n">MultiSequential</span>
<span class="kn">from</span> <span class="nn">espnet.nets.pytorch_backend.transformer.subsampling</span> <span class="kn">import</span> <span class="n">Conv2dSubsampling</span>


<div class="viewcode-block" id="check_and_prepare"><a class="viewcode-back" href="../../../../../_gen/espnet.nets.html#espnet.nets.pytorch_backend.transducer.blocks.check_and_prepare">[docs]</a><span class="k">def</span> <span class="nf">check_and_prepare</span><span class="p">(</span><span class="n">net_part</span><span class="p">,</span> <span class="n">blocks_arch</span><span class="p">,</span> <span class="n">input_layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Check consecutive block shapes match and prepare input parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        net_part (str): either &#39;encoder&#39; or &#39;decoder&#39;</span>
<span class="sd">        blocks_arch (list): list of blocks for network part (type and parameters)</span>
<span class="sd">        input_layer (str): input layer type</span>

<span class="sd">    Return:</span>
<span class="sd">        input_layer (str): input layer type</span>
<span class="sd">        input_layer_odim (int): output dim of input layer</span>
<span class="sd">        input_dropout_rate (float): dropout rate of input layer</span>
<span class="sd">        input_pos_dropout_rate (float): dropout rate of input layer positional enc.</span>
<span class="sd">        out_dim (int): output dim of last block</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">input_dropout_rate</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
        <span class="n">Counter</span><span class="p">(</span>
            <span class="n">b</span><span class="p">[</span><span class="s2">&quot;dropout-rate&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">blocks_arch</span> <span class="k">if</span> <span class="s2">&quot;dropout-rate&quot;</span> <span class="ow">in</span> <span class="n">b</span>
        <span class="p">)</span><span class="o">.</span><span class="n">most_common</span><span class="p">(),</span>
        <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">input_pos_dropout_rate</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
        <span class="n">Counter</span><span class="p">(</span>
            <span class="n">b</span><span class="p">[</span><span class="s2">&quot;pos-dropout-rate&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">blocks_arch</span> <span class="k">if</span> <span class="s2">&quot;pos-dropout-rate&quot;</span> <span class="ow">in</span> <span class="n">b</span>
        <span class="p">)</span><span class="o">.</span><span class="n">most_common</span><span class="p">(),</span>
        <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">input_dropout_rate</span> <span class="o">=</span> <span class="n">input_dropout_rate</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">input_dropout_rate</span> <span class="k">else</span> <span class="mf">0.0</span>
    <span class="n">input_pos_dropout_rate</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">input_pos_dropout_rate</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">input_pos_dropout_rate</span> <span class="k">else</span> <span class="mf">0.0</span>
    <span class="p">)</span>

    <span class="n">cmp_io</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">has_transformer</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">has_conformer</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">blocks_arch</span><span class="p">)):</span>
        <span class="k">if</span> <span class="s2">&quot;type&quot;</span> <span class="ow">in</span> <span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
            <span class="n">block_type</span> <span class="o">=</span> <span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;type&quot;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;type is not defined in the &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;th block.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">block_type</span> <span class="o">==</span> <span class="s2">&quot;transformer&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">{</span><span class="s2">&quot;d_hidden&quot;</span><span class="p">,</span> <span class="s2">&quot;d_ff&quot;</span><span class="p">,</span> <span class="s2">&quot;heads&quot;</span><span class="p">}</span><span class="o">.</span><span class="n">issubset</span><span class="p">(</span><span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Block &quot;</span>
                    <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="o">+</span> <span class="s2">&quot;in &quot;</span>
                    <span class="o">+</span> <span class="n">net_part</span>
                    <span class="o">+</span> <span class="s2">&quot;: Transformer block format is: {&#39;type: transformer&#39;, &quot;</span>
                    <span class="s2">&quot;&#39;d_hidden&#39;: int, &#39;d_ff&#39;: int, &#39;heads&#39;: int, [...]}&quot;</span>
                <span class="p">)</span>

            <span class="n">has_transformer</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">cmp_io</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;d_hidden&quot;</span><span class="p">],</span> <span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;d_hidden&quot;</span><span class="p">]))</span>
        <span class="k">elif</span> <span class="n">block_type</span> <span class="o">==</span> <span class="s2">&quot;conformer&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">net_part</span> <span class="o">!=</span> <span class="s2">&quot;encoder&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Block &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;: conformer type is only for encoder part.&quot;</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="p">{</span>
                <span class="s2">&quot;d_hidden&quot;</span><span class="p">,</span>
                <span class="s2">&quot;d_ff&quot;</span><span class="p">,</span>
                <span class="s2">&quot;heads&quot;</span><span class="p">,</span>
                <span class="s2">&quot;macaron_style&quot;</span><span class="p">,</span>
                <span class="s2">&quot;use_conv_mod&quot;</span><span class="p">,</span>
            <span class="p">}</span><span class="o">.</span><span class="n">issubset</span><span class="p">(</span><span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Block &quot;</span>
                    <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="o">+</span> <span class="s2">&quot; in &quot;</span>
                    <span class="o">+</span> <span class="n">net_part</span>
                    <span class="o">+</span> <span class="s2">&quot;: Conformer block format is {&#39;type: conformer&#39;, &quot;</span>
                    <span class="s2">&quot;&#39;d_hidden&#39;: int, &#39;d_ff&#39;: int, &#39;heads&#39;: int, &quot;</span>
                    <span class="s2">&quot;&#39;macaron_style&#39;: bool, &#39;use_conv_mod&#39;: bool, [...]}&quot;</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;use_conv_mod&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">True</span>
                <span class="ow">and</span> <span class="s2">&quot;conv_mod_kernel&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Block &quot;</span>
                    <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="o">+</span> <span class="s2">&quot;: &#39;use_conv_mod&#39; is True but &#39;use_conv_kernel&#39; is not specified&quot;</span>
                <span class="p">)</span>

            <span class="n">has_conformer</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">cmp_io</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;d_hidden&quot;</span><span class="p">],</span> <span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;d_hidden&quot;</span><span class="p">]))</span>
        <span class="k">elif</span> <span class="n">block_type</span> <span class="o">==</span> <span class="s2">&quot;causal-conv1d&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">{</span><span class="s2">&quot;idim&quot;</span><span class="p">,</span> <span class="s2">&quot;odim&quot;</span><span class="p">,</span> <span class="s2">&quot;kernel_size&quot;</span><span class="p">}</span><span class="o">.</span><span class="n">issubset</span><span class="p">(</span><span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Block &quot;</span>
                    <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="o">+</span> <span class="s2">&quot; in &quot;</span>
                    <span class="o">+</span> <span class="n">net_part</span>
                    <span class="o">+</span> <span class="s2">&quot;: causal conv1d block format is: {&#39;type: causal-conv1d&#39;, &quot;</span>
                    <span class="s2">&quot;&#39;idim&#39;: int, &#39;odim&#39;: int, &#39;kernel_size&#39;: int}&quot;</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">input_layer</span> <span class="o">=</span> <span class="s2">&quot;c-embed&quot;</span>

            <span class="n">cmp_io</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;idim&quot;</span><span class="p">],</span> <span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;odim&quot;</span><span class="p">]))</span>
        <span class="k">elif</span> <span class="n">block_type</span> <span class="o">==</span> <span class="s2">&quot;tdnn&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">{</span><span class="s2">&quot;idim&quot;</span><span class="p">,</span> <span class="s2">&quot;odim&quot;</span><span class="p">,</span> <span class="s2">&quot;ctx_size&quot;</span><span class="p">,</span> <span class="s2">&quot;dilation&quot;</span><span class="p">,</span> <span class="s2">&quot;stride&quot;</span><span class="p">}</span><span class="o">.</span><span class="n">issubset</span><span class="p">(</span>
                <span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Block &quot;</span>
                    <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="o">+</span> <span class="s2">&quot; in &quot;</span>
                    <span class="o">+</span> <span class="n">net_part</span>
                    <span class="o">+</span> <span class="s2">&quot;: TDNN block format is: {&#39;type: tdnn&#39;, &quot;</span>
                    <span class="s2">&quot;&#39;idim&#39;: int, &#39;odim&#39;: int, &#39;ctx_size&#39;: int, &quot;</span>
                    <span class="s2">&quot;&#39;dilation&#39;: int, &#39;stride&#39;: int, [...]}&quot;</span>
                <span class="p">)</span>

            <span class="n">cmp_io</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;idim&quot;</span><span class="p">],</span> <span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;odim&quot;</span><span class="p">]))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;Wrong type for block &quot;</span>
                <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="o">+</span> <span class="s2">&quot; in &quot;</span>
                <span class="o">+</span> <span class="n">net_part</span>
                <span class="o">+</span> <span class="s2">&quot;. Currently supported: &quot;</span>
                <span class="s2">&quot;tdnn, causal-conv1d or transformer&quot;</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">has_transformer</span> <span class="ow">and</span> <span class="n">has_conformer</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="n">net_part</span> <span class="o">+</span> <span class="s2">&quot;: transformer and conformer blocks &quot;</span>
            <span class="s2">&quot;can&#39;t be defined in the same net part.&quot;</span>
        <span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cmp_io</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">cmp_io</span><span class="p">[(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)][</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">cmp_io</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Output/Input mismatch between blocks &quot;</span>
                <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="o">+</span> <span class="s2">&quot; and &quot;</span>
                <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="o">+</span> <span class="s2">&quot; in &quot;</span>
                <span class="o">+</span> <span class="n">net_part</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">blocks_arch</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;type&quot;</span><span class="p">]</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;tdnn&quot;</span><span class="p">,</span> <span class="s2">&quot;causal-conv1d&quot;</span><span class="p">):</span>
        <span class="n">input_layer_odim</span> <span class="o">=</span> <span class="n">blocks_arch</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;idim&quot;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">input_layer_odim</span> <span class="o">=</span> <span class="n">blocks_arch</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;d_hidden&quot;</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">blocks_arch</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;type&quot;</span><span class="p">]</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;tdnn&quot;</span><span class="p">,</span> <span class="s2">&quot;causal-conv1d&quot;</span><span class="p">):</span>
        <span class="n">out_dim</span> <span class="o">=</span> <span class="n">blocks_arch</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;odim&quot;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out_dim</span> <span class="o">=</span> <span class="n">blocks_arch</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;d_hidden&quot;</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">(</span>
        <span class="n">input_layer</span><span class="p">,</span>
        <span class="n">input_layer_odim</span><span class="p">,</span>
        <span class="n">input_dropout_rate</span><span class="p">,</span>
        <span class="n">input_pos_dropout_rate</span><span class="p">,</span>
        <span class="n">out_dim</span><span class="p">,</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="get_pos_enc_and_att_class"><a class="viewcode-back" href="../../../../../_gen/espnet.nets.html#espnet.nets.pytorch_backend.transducer.blocks.get_pos_enc_and_att_class">[docs]</a><span class="k">def</span> <span class="nf">get_pos_enc_and_att_class</span><span class="p">(</span><span class="n">net_part</span><span class="p">,</span> <span class="n">pos_enc_type</span><span class="p">,</span> <span class="n">self_attn_type</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get positional encoding and self attention module class.</span>

<span class="sd">    Args:</span>
<span class="sd">        net_part (str): either &#39;encoder&#39; or &#39;decoder&#39;</span>
<span class="sd">        pos_enc_type (str): positional encoding type</span>
<span class="sd">        self_attn_type (str): self-attention type</span>

<span class="sd">    Return:</span>
<span class="sd">        pos_enc_class (torch.nn.Module): positional encoding class</span>
<span class="sd">        self_attn_class (torch.nn.Module): self-attention class</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pos_enc_type</span> <span class="o">==</span> <span class="s2">&quot;abs_pos&quot;</span><span class="p">:</span>
        <span class="n">pos_enc_class</span> <span class="o">=</span> <span class="n">PositionalEncoding</span>
    <span class="k">elif</span> <span class="n">pos_enc_type</span> <span class="o">==</span> <span class="s2">&quot;scaled_abs_pos&quot;</span><span class="p">:</span>
        <span class="n">pos_enc_class</span> <span class="o">=</span> <span class="n">ScaledPositionalEncoding</span>
    <span class="k">elif</span> <span class="n">pos_enc_type</span> <span class="o">==</span> <span class="s2">&quot;rel_pos&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">net_part</span> <span class="o">==</span> <span class="s2">&quot;encoder&quot;</span> <span class="ow">and</span> <span class="n">self_attn_type</span> <span class="o">!=</span> <span class="s2">&quot;rel_self_attn&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;rel_pos&#39; is only compatible with &#39;rel_self_attn&#39;&quot;</span><span class="p">)</span>
        <span class="n">pos_enc_class</span> <span class="o">=</span> <span class="n">RelPositionalEncoding</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;pos_enc_type should be either &#39;abs_pos&#39;, &#39;scaled_abs_pos&#39; or &#39;rel_pos&#39;&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">self_attn_type</span> <span class="o">==</span> <span class="s2">&quot;rel_self_attn&quot;</span><span class="p">:</span>
        <span class="n">self_attn_class</span> <span class="o">=</span> <span class="n">RelPositionMultiHeadedAttention</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">self_attn_class</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span>

    <span class="k">return</span> <span class="n">pos_enc_class</span><span class="p">,</span> <span class="n">self_attn_class</span></div>


<div class="viewcode-block" id="build_input_layer"><a class="viewcode-back" href="../../../../../_gen/espnet.nets.html#espnet.nets.pytorch_backend.transducer.blocks.build_input_layer">[docs]</a><span class="k">def</span> <span class="nf">build_input_layer</span><span class="p">(</span>
    <span class="n">input_layer</span><span class="p">,</span>
    <span class="n">idim</span><span class="p">,</span>
    <span class="n">odim</span><span class="p">,</span>
    <span class="n">pos_enc_class</span><span class="p">,</span>
    <span class="n">dropout_rate_embed</span><span class="p">,</span>
    <span class="n">dropout_rate</span><span class="p">,</span>
    <span class="n">pos_dropout_rate</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build input layer.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_layer (str): input layer type</span>
<span class="sd">        idim (int): input dimension</span>
<span class="sd">        odim (int): output dimension</span>
<span class="sd">        pos_enc_class (class): positional encoding class</span>
<span class="sd">        dropout_rate_embed (float): dropout rate for embedding layer</span>
<span class="sd">        dropout_rate (float): dropout rate for input layer</span>
<span class="sd">        pos_dropout_rate (float): dropout rate for positional encoding</span>
<span class="sd">        padding_idx (int): padding index for embedding input layer (if specified)</span>

<span class="sd">    Returns:</span>
<span class="sd">        (torch.nn.*): input layer module</span>
<span class="sd">        subsampling_factor (int): subsampling factor</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pos_enc_class</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;RelPositionalEncoding&quot;</span><span class="p">:</span>
        <span class="n">pos_enc_class_subsampling</span> <span class="o">=</span> <span class="n">pos_enc_class</span><span class="p">(</span><span class="n">odim</span><span class="p">,</span> <span class="n">pos_dropout_rate</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pos_enc_class_subsampling</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">input_layer</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">idim</span><span class="p">,</span> <span class="n">odim</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">odim</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">pos_enc_class</span><span class="p">(</span><span class="n">odim</span><span class="p">,</span> <span class="n">pos_dropout_rate</span><span class="p">),</span>
            <span class="p">),</span>
            <span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">input_layer</span> <span class="o">==</span> <span class="s2">&quot;conv2d&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Conv2dSubsampling</span><span class="p">(</span><span class="n">idim</span><span class="p">,</span> <span class="n">odim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">pos_enc_class_subsampling</span><span class="p">),</span> <span class="mi">4</span>
    <span class="k">elif</span> <span class="n">input_layer</span> <span class="o">==</span> <span class="s2">&quot;vgg2l&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">VGG2L</span><span class="p">(</span><span class="n">idim</span><span class="p">,</span> <span class="n">odim</span><span class="p">,</span> <span class="n">pos_enc_class_subsampling</span><span class="p">),</span> <span class="mi">4</span>
    <span class="k">elif</span> <span class="n">input_layer</span> <span class="o">==</span> <span class="s2">&quot;embed&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">idim</span><span class="p">,</span> <span class="n">odim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span><span class="p">),</span>
                <span class="n">pos_enc_class</span><span class="p">(</span><span class="n">odim</span><span class="p">,</span> <span class="n">pos_dropout_rate</span><span class="p">),</span>
            <span class="p">),</span>
            <span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">input_layer</span> <span class="o">==</span> <span class="s2">&quot;c-embed&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">idim</span><span class="p">,</span> <span class="n">odim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate_embed</span><span class="p">),</span>
            <span class="p">),</span>
            <span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Support: linear, conv2d, vgg2l and embed&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="build_transformer_block"><a class="viewcode-back" href="../../../../../_gen/espnet.nets.html#espnet.nets.pytorch_backend.transducer.blocks.build_transformer_block">[docs]</a><span class="k">def</span> <span class="nf">build_transformer_block</span><span class="p">(</span><span class="n">net_part</span><span class="p">,</span> <span class="n">block_arch</span><span class="p">,</span> <span class="n">pw_layer_type</span><span class="p">,</span> <span class="n">pw_activation_type</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build function for transformer block.</span>

<span class="sd">    Args:</span>
<span class="sd">        net_part (str): either &#39;encoder&#39; or &#39;decoder&#39;</span>
<span class="sd">        block_arch (dict): transformer block parameters</span>
<span class="sd">        pw_layer_type (str): positionwise layer type</span>
<span class="sd">        pw_activation_type (str): positionwise activation type</span>

<span class="sd">    Returns:</span>
<span class="sd">        (function): function to create transformer block</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">d_hidden</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;d_hidden&quot;</span><span class="p">]</span>
    <span class="n">d_ff</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;d_ff&quot;</span><span class="p">]</span>
    <span class="n">heads</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;heads&quot;</span><span class="p">]</span>

    <span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;dropout-rate&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;dropout-rate&quot;</span> <span class="ow">in</span> <span class="n">block_arch</span> <span class="k">else</span> <span class="mf">0.0</span>
    <span class="n">pos_dropout_rate</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;pos-dropout-rate&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;pos-dropout-rate&quot;</span> <span class="ow">in</span> <span class="n">block_arch</span> <span class="k">else</span> <span class="mf">0.0</span>
    <span class="p">)</span>
    <span class="n">att_dropout_rate</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;att-dropout-rate&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;att-dropout-rate&quot;</span> <span class="ow">in</span> <span class="n">block_arch</span> <span class="k">else</span> <span class="mf">0.0</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">pw_layer_type</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
        <span class="n">pw_layer</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span>
        <span class="n">pw_activation</span> <span class="o">=</span> <span class="n">get_activation</span><span class="p">(</span><span class="n">pw_activation_type</span><span class="p">)</span>
        <span class="n">pw_layer_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">d_hidden</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">pos_dropout_rate</span><span class="p">,</span> <span class="n">pw_activation</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Transformer block only supports linear yet.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">net_part</span> <span class="o">==</span> <span class="s2">&quot;encoder&quot;</span><span class="p">:</span>
        <span class="n">transformer_layer_class</span> <span class="o">=</span> <span class="n">EncoderLayer</span>
    <span class="k">elif</span> <span class="n">net_part</span> <span class="o">==</span> <span class="s2">&quot;decoder&quot;</span><span class="p">:</span>
        <span class="n">transformer_layer_class</span> <span class="o">=</span> <span class="n">DecoderLayer</span>

    <span class="k">return</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">transformer_layer_class</span><span class="p">(</span>
        <span class="n">d_hidden</span><span class="p">,</span>
        <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="n">heads</span><span class="p">,</span> <span class="n">d_hidden</span><span class="p">,</span> <span class="n">att_dropout_rate</span><span class="p">),</span>
        <span class="n">pw_layer</span><span class="p">(</span><span class="o">*</span><span class="n">pw_layer_args</span><span class="p">),</span>
        <span class="n">dropout_rate</span><span class="p">,</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="build_conformer_block"><a class="viewcode-back" href="../../../../../_gen/espnet.nets.html#espnet.nets.pytorch_backend.transducer.blocks.build_conformer_block">[docs]</a><span class="k">def</span> <span class="nf">build_conformer_block</span><span class="p">(</span>
    <span class="n">block_arch</span><span class="p">,</span>
    <span class="n">self_attn_class</span><span class="p">,</span>
    <span class="n">pw_layer_type</span><span class="p">,</span>
    <span class="n">pw_activation_type</span><span class="p">,</span>
    <span class="n">conv_mod_activation_type</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build function for conformer block.</span>

<span class="sd">    Args:</span>
<span class="sd">        block_arch (dict): conformer block parameters</span>
<span class="sd">        self_attn_type (str): self-attention module type</span>
<span class="sd">        pw_layer_type (str): positionwise layer type</span>
<span class="sd">        pw_activation_type (str): positionwise activation type</span>
<span class="sd">        conv_mod_activation_type (str): convolutional module activation type</span>

<span class="sd">    Returns:</span>
<span class="sd">        (function): function to create conformer block</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">d_hidden</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;d_hidden&quot;</span><span class="p">]</span>
    <span class="n">d_ff</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;d_ff&quot;</span><span class="p">]</span>
    <span class="n">heads</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;heads&quot;</span><span class="p">]</span>
    <span class="n">macaron_style</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;macaron_style&quot;</span><span class="p">]</span>
    <span class="n">use_conv_mod</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;use_conv_mod&quot;</span><span class="p">]</span>

    <span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;dropout-rate&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;dropout-rate&quot;</span> <span class="ow">in</span> <span class="n">block_arch</span> <span class="k">else</span> <span class="mf">0.0</span>
    <span class="n">pos_dropout_rate</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;pos-dropout-rate&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;pos-dropout-rate&quot;</span> <span class="ow">in</span> <span class="n">block_arch</span> <span class="k">else</span> <span class="mf">0.0</span>
    <span class="p">)</span>
    <span class="n">att_dropout_rate</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;att-dropout-rate&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;att-dropout-rate&quot;</span> <span class="ow">in</span> <span class="n">block_arch</span> <span class="k">else</span> <span class="mf">0.0</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">pw_layer_type</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
        <span class="n">pw_layer</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span>
        <span class="n">pw_activation</span> <span class="o">=</span> <span class="n">get_activation</span><span class="p">(</span><span class="n">pw_activation_type</span><span class="p">)</span>
        <span class="n">pw_layer_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">d_hidden</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">pos_dropout_rate</span><span class="p">,</span> <span class="n">pw_activation</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Conformer block only supports linear yet.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">use_conv_mod</span><span class="p">:</span>
        <span class="n">conv_layer</span> <span class="o">=</span> <span class="n">ConvolutionModule</span>
        <span class="n">conv_activation</span> <span class="o">=</span> <span class="n">get_activation</span><span class="p">(</span><span class="n">conv_mod_activation_type</span><span class="p">)</span>
        <span class="n">conv_layers_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">d_hidden</span><span class="p">,</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;conv_mod_kernel&quot;</span><span class="p">],</span> <span class="n">conv_activation</span><span class="p">)</span>

    <span class="k">return</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">ConformerEncoderLayer</span><span class="p">(</span>
        <span class="n">d_hidden</span><span class="p">,</span>
        <span class="n">self_attn_class</span><span class="p">(</span><span class="n">heads</span><span class="p">,</span> <span class="n">d_hidden</span><span class="p">,</span> <span class="n">att_dropout_rate</span><span class="p">),</span>
        <span class="n">pw_layer</span><span class="p">(</span><span class="o">*</span><span class="n">pw_layer_args</span><span class="p">),</span>
        <span class="n">pw_layer</span><span class="p">(</span><span class="o">*</span><span class="n">pw_layer_args</span><span class="p">)</span> <span class="k">if</span> <span class="n">macaron_style</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">conv_layer</span><span class="p">(</span><span class="o">*</span><span class="n">conv_layers_args</span><span class="p">)</span> <span class="k">if</span> <span class="n">use_conv_mod</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dropout_rate</span><span class="p">,</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="build_causal_conv1d_block"><a class="viewcode-back" href="../../../../../_gen/espnet.nets.html#espnet.nets.pytorch_backend.transducer.blocks.build_causal_conv1d_block">[docs]</a><span class="k">def</span> <span class="nf">build_causal_conv1d_block</span><span class="p">(</span><span class="n">block_arch</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build function for causal conv1d block.</span>

<span class="sd">    Args:</span>
<span class="sd">        block_arch (dict): causal conv1d block parameters</span>

<span class="sd">    Returns:</span>
<span class="sd">        (function): function to create causal conv1d block</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">idim</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;idim&quot;</span><span class="p">]</span>
    <span class="n">odim</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;odim&quot;</span><span class="p">]</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">]</span>

    <span class="k">return</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">CausalConv1d</span><span class="p">(</span><span class="n">idim</span><span class="p">,</span> <span class="n">odim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="build_tdnn_block"><a class="viewcode-back" href="../../../../../_gen/espnet.nets.html#espnet.nets.pytorch_backend.transducer.blocks.build_tdnn_block">[docs]</a><span class="k">def</span> <span class="nf">build_tdnn_block</span><span class="p">(</span><span class="n">block_arch</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build function for tdnn block.</span>

<span class="sd">    Args:</span>
<span class="sd">        block_arch (dict): tdnn block parameters</span>

<span class="sd">    Returns:</span>
<span class="sd">        (function): function to create tdnn block</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">idim</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;idim&quot;</span><span class="p">]</span>
    <span class="n">odim</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;odim&quot;</span><span class="p">]</span>
    <span class="n">ctx_size</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;ctx_size&quot;</span><span class="p">]</span>
    <span class="n">dilation</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;dilation&quot;</span><span class="p">]</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;stride&quot;</span><span class="p">]</span>

    <span class="n">use_batch_norm</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;use-batch-norm&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;use-batch-norm&quot;</span> <span class="ow">in</span> <span class="n">block_arch</span> <span class="k">else</span> <span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">use_relu</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;use-relu&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;use-relu&quot;</span> <span class="ow">in</span> <span class="n">block_arch</span> <span class="k">else</span> <span class="kc">False</span>

    <span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">block_arch</span><span class="p">[</span><span class="s2">&quot;dropout-rate&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;dropout-rate&quot;</span> <span class="ow">in</span> <span class="n">block_arch</span> <span class="k">else</span> <span class="mf">0.0</span>

    <span class="k">return</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">TDNN</span><span class="p">(</span>
        <span class="n">idim</span><span class="p">,</span>
        <span class="n">odim</span><span class="p">,</span>
        <span class="n">ctx_size</span><span class="o">=</span><span class="n">ctx_size</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span>
        <span class="n">batch_norm</span><span class="o">=</span><span class="n">use_batch_norm</span><span class="p">,</span>
        <span class="n">relu</span><span class="o">=</span><span class="n">use_relu</span><span class="p">,</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="build_blocks"><a class="viewcode-back" href="../../../../../_gen/espnet.nets.html#espnet.nets.pytorch_backend.transducer.blocks.build_blocks">[docs]</a><span class="k">def</span> <span class="nf">build_blocks</span><span class="p">(</span>
    <span class="n">net_part</span><span class="p">,</span>
    <span class="n">idim</span><span class="p">,</span>
    <span class="n">input_layer</span><span class="p">,</span>
    <span class="n">blocks_arch</span><span class="p">,</span>
    <span class="n">repeat_block</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">self_attn_type</span><span class="o">=</span><span class="s2">&quot;self_attn&quot;</span><span class="p">,</span>
    <span class="n">positional_encoding_type</span><span class="o">=</span><span class="s2">&quot;abs_pos&quot;</span><span class="p">,</span>
    <span class="n">positionwise_layer_type</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
    <span class="n">positionwise_activation_type</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">conv_mod_activation_type</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">dropout_rate_embed</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build block for customizable architecture.</span>

<span class="sd">    Args:</span>
<span class="sd">        net_part (str): either &#39;encoder&#39; or &#39;decoder&#39;</span>
<span class="sd">        idim (int): dimension of inputs</span>
<span class="sd">        input_layer (str): input layer type</span>
<span class="sd">        blocks_arch (list): list of blocks for network part (type and parameters)</span>
<span class="sd">        repeat_block (int): repeat provided blocks N times if N &gt; 1</span>
<span class="sd">        positional_encoding_type (str): positional encoding layer type</span>
<span class="sd">        positionwise_layer_type (str): linear</span>
<span class="sd">        positionwise_activation_type (str): positionwise activation type</span>
<span class="sd">        conv_mod_activation_type (str): convolutional module activation type</span>
<span class="sd">        dropout_rate_embed (float): dropout rate for embedding</span>
<span class="sd">        padding_idx (int): padding index for embedding input layer (if specified)</span>

<span class="sd">    Returns:</span>
<span class="sd">        in_layer (torch.nn.*): input layer</span>
<span class="sd">        all_blocks (MultiSequential): all blocks for network part</span>
<span class="sd">        out_dim (int): dimension of last block output</span>
<span class="sd">        conv_subsampling_factor (int): subsampling factor in frontend CNN</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fn_modules</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="p">(</span>
        <span class="n">input_layer</span><span class="p">,</span>
        <span class="n">input_layer_odim</span><span class="p">,</span>
        <span class="n">input_dropout_rate</span><span class="p">,</span>
        <span class="n">input_pos_dropout_rate</span><span class="p">,</span>
        <span class="n">out_dim</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">=</span> <span class="n">check_and_prepare</span><span class="p">(</span><span class="n">net_part</span><span class="p">,</span> <span class="n">blocks_arch</span><span class="p">,</span> <span class="n">input_layer</span><span class="p">)</span>

    <span class="n">pos_enc_class</span><span class="p">,</span> <span class="n">self_attn_class</span> <span class="o">=</span> <span class="n">get_pos_enc_and_att_class</span><span class="p">(</span>
        <span class="n">net_part</span><span class="p">,</span> <span class="n">positional_encoding_type</span><span class="p">,</span> <span class="n">self_attn_type</span>
    <span class="p">)</span>

    <span class="n">in_layer</span><span class="p">,</span> <span class="n">conv_subsampling_factor</span> <span class="o">=</span> <span class="n">build_input_layer</span><span class="p">(</span>
        <span class="n">input_layer</span><span class="p">,</span>
        <span class="n">idim</span><span class="p">,</span>
        <span class="n">input_layer_odim</span><span class="p">,</span>
        <span class="n">pos_enc_class</span><span class="p">,</span>
        <span class="n">dropout_rate_embed</span><span class="p">,</span>
        <span class="n">input_dropout_rate</span><span class="p">,</span>
        <span class="n">input_pos_dropout_rate</span><span class="p">,</span>
        <span class="n">padding_idx</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">blocks_arch</span><span class="p">)):</span>
        <span class="n">block_type</span> <span class="o">=</span> <span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;type&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">block_type</span> <span class="o">==</span> <span class="s2">&quot;tdnn&quot;</span><span class="p">:</span>
            <span class="n">module</span> <span class="o">=</span> <span class="n">build_tdnn_block</span><span class="p">(</span><span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">elif</span> <span class="n">block_type</span> <span class="o">==</span> <span class="s2">&quot;transformer&quot;</span><span class="p">:</span>
            <span class="n">module</span> <span class="o">=</span> <span class="n">build_transformer_block</span><span class="p">(</span>
                <span class="n">net_part</span><span class="p">,</span>
                <span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                <span class="n">positionwise_layer_type</span><span class="p">,</span>
                <span class="n">positionwise_activation_type</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">block_type</span> <span class="o">==</span> <span class="s2">&quot;conformer&quot;</span><span class="p">:</span>
            <span class="n">module</span> <span class="o">=</span> <span class="n">build_conformer_block</span><span class="p">(</span>
                <span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                <span class="n">self_attn_class</span><span class="p">,</span>
                <span class="n">positionwise_layer_type</span><span class="p">,</span>
                <span class="n">positionwise_activation_type</span><span class="p">,</span>
                <span class="n">conv_mod_activation_type</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">block_type</span> <span class="o">==</span> <span class="s2">&quot;causal-conv1d&quot;</span><span class="p">:</span>
            <span class="n">module</span> <span class="o">=</span> <span class="n">build_causal_conv1d_block</span><span class="p">(</span><span class="n">blocks_arch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

        <span class="n">fn_modules</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">repeat_block</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">fn_modules</span> <span class="o">=</span> <span class="n">fn_modules</span> <span class="o">*</span> <span class="n">repeat_block</span>

    <span class="k">return</span> <span class="p">(</span>
        <span class="n">in_layer</span><span class="p">,</span>
        <span class="n">MultiSequential</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">fn</span><span class="p">()</span> <span class="k">for</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">fn_modules</span><span class="p">]),</span>
        <span class="n">out_dim</span><span class="p">,</span>
        <span class="n">conv_subsampling_factor</span><span class="p">,</span>
    <span class="p">)</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2017, Shinji Watanabe.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>