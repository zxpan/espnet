

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>espnet2.asr package &mdash; ESPnet 0.9.8 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="espnet2.fileio package" href="espnet2.fileio.html" />
    <link rel="prev" title="espnet2.main_funcs package" href="espnet2.main_funcs.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> ESPnet
          

          
          </a>

          
            
            
              <div class="version">
                0.9.8
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Tutorial:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallelization.html">Using Job scheduling system</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docker.html">Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">ESPnet2:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_tutorial.html">ESPnet2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_tutorial.html#instruction-for-run-sh">Instruction for run.sh</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_training_option.html">Change the configuration for training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_task.html">Task class and data input system for training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_distributed.html">Distributed training</a></li>
</ul>
<p class="caption"><span class="caption-text">Notebook:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notebook/asr_cli.html">Speech Recognition (Recipe)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/asr_library.html">Speech Recognition (Library)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/tts_cli.html">Text-to-Speech (Recipe)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/pretrained.html">Pretrained Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/tts_realtime_demo.html">ESPnet real time E2E-TTS demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/st_demo.html">ESPnet Speech Translation Demonstration</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="espnet.st.html">espnet.st package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.lm.html">espnet.lm package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.tts.html">espnet.tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.transform.html">espnet.transform package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.bin.html">espnet.bin package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.scheduler.html">espnet.scheduler package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.vc.html">espnet.vc package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.utils.html">espnet.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.optimizer.html">espnet.optimizer package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.nets.html">espnet.nets package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.mt.html">espnet.mt package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.asr.html">espnet.asr package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.tasks.html">espnet2.tasks package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.text.html">espnet2.text package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.optimizers.html">espnet2.optimizers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.torch_utils.html">espnet2.torch_utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.iterators.html">espnet2.iterators package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.lm.html">espnet2.lm package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.tts.html">espnet2.tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.bin.html">espnet2.bin package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.layers.html">espnet2.layers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.schedulers.html">espnet2.schedulers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.enh.html">espnet2.enh package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.utils.html">espnet2.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.main_funcs.html">espnet2.main_funcs package</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">espnet2.asr package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-espnet-model">espnet2.asr.espnet_model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-ctc">espnet2.asr.ctc</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-init">espnet2.asr.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-conformer-encoder">espnet2.asr.encoder.conformer_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-contextual-block-transformer-encoder">espnet2.asr.encoder.contextual_block_transformer_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-rnn-encoder">espnet2.asr.encoder.rnn_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-transformer-encoder">espnet2.asr.encoder.transformer_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-vgg-rnn-encoder">espnet2.asr.encoder.vgg_rnn_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-init">espnet2.asr.encoder.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-wav2vec2-encoder">espnet2.asr.encoder.wav2vec2_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-extract-features">espnet2.asr.encoder.extract_features</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-abs-encoder">espnet2.asr.encoder.abs_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-frontend-windowing">espnet2.asr.frontend.windowing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-frontend-default">espnet2.asr.frontend.default</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-frontend-init">espnet2.asr.frontend.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-frontend-abs-frontend">espnet2.asr.frontend.abs_frontend</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-decoder-rnn-decoder">espnet2.asr.decoder.rnn_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-decoder-abs-decoder">espnet2.asr.decoder.abs_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-decoder-transformer-decoder">espnet2.asr.decoder.transformer_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-decoder-init">espnet2.asr.decoder.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-specaug-specaug">espnet2.asr.specaug.specaug</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-specaug-abs-specaug">espnet2.asr.specaug.abs_specaug</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-specaug-init">espnet2.asr.specaug.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-preencoder-sinc">espnet2.asr.preencoder.sinc</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-preencoder-abs-preencoder">espnet2.asr.preencoder.abs_preencoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-preencoder-init">espnet2.asr.preencoder.__init__</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.fileio.html">espnet2.fileio package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.samplers.html">espnet2.samplers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.train.html">espnet2.train package</a></li>
</ul>
<p class="caption"><span class="caption-text">Tool Reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../apis/espnet_bin.html">core tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/espnet2_bin.html">core tools (espnet2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/utils_py.html">python utility tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/utils_sh.html">bash utility tools</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">ESPnet</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>espnet2.asr package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/_gen/espnet2.asr.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="espnet2-asr-package">
<h1>espnet2.asr package<a class="headerlink" href="#espnet2-asr-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="espnet2-asr-espnet-model">
<span id="id1"></span><h2>espnet2.asr.espnet_model<a class="headerlink" href="#espnet2-asr-espnet-model" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.espnet_model"></span><dl class="class">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.espnet_model.</code><code class="sig-name descname">ESPnetASRModel</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int, token_list: Union[Tuple[str, ...], List[str]], frontend: Optional[espnet2.asr.frontend.abs_frontend.AbsFrontend], specaug: Optional[espnet2.asr.specaug.abs_specaug.AbsSpecAug], normalize: Optional[espnet2.layers.abs_normalize.AbsNormalize], preencoder: Optional[espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder], encoder: espnet2.asr.encoder.abs_encoder.AbsEncoder, decoder: espnet2.asr.decoder.abs_decoder.AbsDecoder, ctc: espnet2.asr.ctc.CTC, rnnt_decoder: None, ctc_weight: float = 0.5, ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_cer: bool = True, report_wer: bool = True, sym_space: str = '&lt;space&gt;', sym_blank: str = '&lt;blank&gt;'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="espnet2.train.html#espnet2.train.abs_espnet_model.AbsESPnetModel" title="espnet2.train.abs_espnet_model.AbsESPnetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.train.abs_espnet_model.AbsESPnetModel</span></code></a></p>
<p>CTC-attention hybrid Encoder-Decoder model</p>
<dl class="method">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel.collect_feats">
<code class="sig-name descname">collect_feats</code><span class="sig-paren">(</span><em class="sig-param">speech: torch.Tensor</em>, <em class="sig-param">speech_lengths: torch.Tensor</em>, <em class="sig-param">text: torch.Tensor</em>, <em class="sig-param">text_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Dict[str, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel.collect_feats"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel.collect_feats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel.encode">
<code class="sig-name descname">encode</code><span class="sig-paren">(</span><em class="sig-param">speech: torch.Tensor</em>, <em class="sig-param">speech_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel.encode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel.encode" title="Permalink to this definition">¶</a></dt>
<dd><p>Frontend + Encoder. Note that this method is used by asr_inference.py</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>speech</strong> – (Batch, Length, …)</p></li>
<li><p><strong>speech_lengths</strong> – (Batch, )</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">speech: torch.Tensor</em>, <em class="sig-param">speech_lengths: torch.Tensor</em>, <em class="sig-param">text: torch.Tensor</em>, <em class="sig-param">text_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Frontend + Encoder + Decoder + Calc loss</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>speech</strong> – (Batch, Length, …)</p></li>
<li><p><strong>speech_lengths</strong> – (Batch, )</p></li>
<li><p><strong>text</strong> – (Batch, Length)</p></li>
<li><p><strong>text_lengths</strong> – (Batch,)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet2-asr-ctc">
<span id="id2"></span><h2>espnet2.asr.ctc<a class="headerlink" href="#espnet2-asr-ctc" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.ctc"></span><dl class="class">
<dt id="espnet2.asr.ctc.CTC">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.ctc.</code><code class="sig-name descname">CTC</code><span class="sig-paren">(</span><em class="sig-param">odim: int</em>, <em class="sig-param">encoder_output_sizse: int</em>, <em class="sig-param">dropout_rate: float = 0.0</em>, <em class="sig-param">ctc_type: str = 'builtin'</em>, <em class="sig-param">reduce: bool = True</em>, <em class="sig-param">ignore_nan_grad: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/ctc.html#CTC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.ctc.CTC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>CTC module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>odim</strong> – dimension of outputs</p></li>
<li><p><strong>encoder_output_sizse</strong> – number of encoder projection units</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate (0.0 ~ 1.0)</p></li>
<li><p><strong>ctc_type</strong> – builtin or warpctc</p></li>
<li><p><strong>reduce</strong> – reduce the CTC loss into a scalar</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.ctc.CTC.argmax">
<code class="sig-name descname">argmax</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/ctc.html#CTC.argmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.ctc.CTC.argmax" title="Permalink to this definition">¶</a></dt>
<dd><p>argmax of frame activations</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hs_pad</strong> (<em>torch.Tensor</em>) – 3d tensor (B, Tmax, eprojs)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>argmax applied 2d tensor (B, Tmax)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.ctc.CTC.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em>, <em class="sig-param">hlens</em>, <em class="sig-param">ys_pad</em>, <em class="sig-param">ys_lens</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/ctc.html#CTC.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.ctc.CTC.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate CTC loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hs_pad</strong> – batch of padded hidden state sequences (B, Tmax, D)</p></li>
<li><p><strong>hlens</strong> – batch of lengths of hidden state sequences (B)</p></li>
<li><p><strong>ys_pad</strong> – batch of padded character id sequence tensor (B, Lmax)</p></li>
<li><p><strong>ys_lens</strong> – batch of lengths of character sequence (B)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.ctc.CTC.log_softmax">
<code class="sig-name descname">log_softmax</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/ctc.html#CTC.log_softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.ctc.CTC.log_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>log_softmax of frame activations</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hs_pad</strong> (<em>Tensor</em>) – 3d tensor (B, Tmax, eprojs)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>log softmax applied 3d tensor (B, Tmax, odim)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.ctc.CTC.loss_fn">
<code class="sig-name descname">loss_fn</code><span class="sig-paren">(</span><em class="sig-param">th_pred</em>, <em class="sig-param">th_target</em>, <em class="sig-param">th_ilen</em>, <em class="sig-param">th_olen</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/asr/ctc.html#CTC.loss_fn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.ctc.CTC.loss_fn" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet2-asr-init">
<span id="id3"></span><h2>espnet2.asr.__init__<a class="headerlink" href="#espnet2-asr-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.__init__"></span></div>
<div class="section" id="espnet2-asr-encoder-conformer-encoder">
<span id="id4"></span><h2>espnet2.asr.encoder.conformer_encoder<a class="headerlink" href="#espnet2-asr-encoder-conformer-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.conformer_encoder"></span><p>Conformer encoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.encoder.conformer_encoder.ConformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.conformer_encoder.</code><code class="sig-name descname">ConformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'conv2d'</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">positionwise_layer_type: str = 'linear'</em>, <em class="sig-param">positionwise_conv_kernel_size: int = 3</em>, <em class="sig-param">macaron_style: bool = False</em>, <em class="sig-param">pos_enc_layer_type: str = 'rel_pos'</em>, <em class="sig-param">selfattention_layer_type: str = 'rel_selfattn'</em>, <em class="sig-param">activation_type: str = 'swish'</em>, <em class="sig-param">use_cnn_module: bool = True</em>, <em class="sig-param">cnn_module_kernel: int = 31</em>, <em class="sig-param">padding_idx: int = -1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/conformer_encoder.html#ConformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.conformer_encoder.ConformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>Conformer encoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>int</em>) – Input dimension.</p></li>
<li><p><strong>output_size</strong> (<em>int</em>) – Dimention of attention.</p></li>
<li><p><strong>attention_heads</strong> (<em>int</em>) – The number of heads of multi head attention.</p></li>
<li><p><strong>linear_units</strong> (<em>int</em>) – The number of units of position-wise feed forward.</p></li>
<li><p><strong>num_blocks</strong> (<em>int</em>) – The number of decoder blocks.</p></li>
<li><p><strong>dropout_rate</strong> (<em>float</em>) – Dropout rate.</p></li>
<li><p><strong>attention_dropout_rate</strong> (<em>float</em>) – Dropout rate in attention.</p></li>
<li><p><strong>positional_dropout_rate</strong> (<em>float</em>) – Dropout rate after adding positional encoding.</p></li>
<li><p><strong>input_layer</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>torch.nn.Module</em><em>]</em>) – Input layer type.</p></li>
<li><p><strong>normalize_before</strong> (<em>bool</em>) – Whether to use layer_norm before the first block.</p></li>
<li><p><strong>concat_after</strong> (<em>bool</em>) – Whether to concat attention layer’s input and output.
If True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
If False, no additional linear will be applied. i.e. x -&gt; x + att(x)</p></li>
<li><p><strong>positionwise_layer_type</strong> (<em>str</em>) – “linear”, “conv1d”, or “conv1d-linear”.</p></li>
<li><p><strong>positionwise_conv_kernel_size</strong> (<em>int</em>) – Kernel size of positionwise conv1d layer.</p></li>
<li><p><strong>encoder_pos_enc_layer_type</strong> (<em>str</em>) – Encoder positional encoding layer type.</p></li>
<li><p><strong>encoder_attn_layer_type</strong> (<em>str</em>) – Encoder attention layer type.</p></li>
<li><p><strong>activation_type</strong> (<em>str</em>) – Encoder activation function type.</p></li>
<li><p><strong>macaron_style</strong> (<em>bool</em>) – Whether to use macaron style for positionwise layer.</p></li>
<li><p><strong>use_cnn_module</strong> (<em>bool</em>) – Whether to use convolution module.</p></li>
<li><p><strong>cnn_module_kernel</strong> (<em>int</em>) – Kernerl size of convolution module.</p></li>
<li><p><strong>padding_idx</strong> (<em>int</em>) – Padding idx for input_layer=embed.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.conformer_encoder.ConformerEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/conformer_encoder.html#ConformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.conformer_encoder.ConformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate forward propagation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> (<em>torch.Tensor</em>) – Input tensor (#batch, L, input_size).</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – Input length (#batch).</p></li>
<li><p><strong>prev_states</strong> (<em>torch.Tensor</em>) – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output tensor (#batch, L, output_size).
torch.Tensor: Output length (#batch).
torch.Tensor: Not to be used now.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.conformer_encoder.ConformerEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/conformer_encoder.html#ConformerEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.conformer_encoder.ConformerEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet2-asr-encoder-contextual-block-transformer-encoder">
<span id="id5"></span><h2>espnet2.asr.encoder.contextual_block_transformer_encoder<a class="headerlink" href="#espnet2-asr-encoder-contextual-block-transformer-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.contextual_block_transformer_encoder"></span><p>Encoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.contextual_block_transformer_encoder.</code><code class="sig-name descname">ContextualBlockTransformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: Optional[str] = 'conv2d'</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">positionwise_layer_type: str = 'linear'</em>, <em class="sig-param">positionwise_conv_kernel_size: int = 1</em>, <em class="sig-param">padding_idx: int = -1</em>, <em class="sig-param">block_size: int = 40</em>, <em class="sig-param">hop_size: int = 16</em>, <em class="sig-param">look_ahead: int = 16</em>, <em class="sig-param">init_average: bool = True</em>, <em class="sig-param">ctx_pos_enc: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_transformer_encoder.html#ContextualBlockTransformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>Contextual Block Transformer encoder module.</p>
<p>Details in Tsunoo et al. “Transformer ASR with contextual block processing”
(<a class="reference external" href="https://arxiv.org/abs/1910.07204">https://arxiv.org/abs/1910.07204</a>)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – input dim</p></li>
<li><p><strong>output_size</strong> – dimension of attention</p></li>
<li><p><strong>attention_heads</strong> – the number of heads of multi head attention</p></li>
<li><p><strong>linear_units</strong> – the number of units of position-wise feed forward</p></li>
<li><p><strong>num_blocks</strong> – the number of decoder blocks</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate</p></li>
<li><p><strong>attention_dropout_rate</strong> – dropout rate in attention</p></li>
<li><p><strong>positional_dropout_rate</strong> – dropout rate after adding positional encoding</p></li>
<li><p><strong>input_layer</strong> – input layer type</p></li>
<li><p><strong>pos_enc_class</strong> – PositionalEncoding or ScaledPositionalEncoding</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
<li><p><strong>concat_after</strong> – whether to concat attention layer’s input and output
if True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
if False, no additional linear will be applied.
i.e. x -&gt; x + att(x)</p></li>
<li><p><strong>positionwise_layer_type</strong> – linear of conv1d</p></li>
<li><p><strong>positionwise_conv_kernel_size</strong> – kernel size of positionwise conv1d layer</p></li>
<li><p><strong>padding_idx</strong> – padding_idx for input_layer=embed</p></li>
<li><p><strong>block_size</strong> – block size for contextual block processing</p></li>
<li><p><strong>hop_Size</strong> – hop size for block processing</p></li>
<li><p><strong>look_ahead</strong> – look-ahead size for block_processing</p></li>
<li><p><strong>init_average</strong> – whether to use average as initial context (otherwise max values)</p></li>
<li><p><strong>ctx_pos_enc</strong> – whether to use positional encoding to the context vectors</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_transformer_encoder.html#ContextualBlockTransformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Embed positions in tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_transformer_encoder.html#ContextualBlockTransformerEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet2-asr-encoder-rnn-encoder">
<span id="id6"></span><h2>espnet2.asr.encoder.rnn_encoder<a class="headerlink" href="#espnet2-asr-encoder-rnn-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.rnn_encoder"></span><dl class="class">
<dt id="espnet2.asr.encoder.rnn_encoder.RNNEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.rnn_encoder.</code><code class="sig-name descname">RNNEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">rnn_type: str = 'lstm'</em>, <em class="sig-param">bidirectional: bool = True</em>, <em class="sig-param">use_projection: bool = True</em>, <em class="sig-param">num_layers: int = 4</em>, <em class="sig-param">hidden_size: int = 320</em>, <em class="sig-param">output_size: int = 320</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">subsample: Optional[Sequence[int]] = (2</em>, <em class="sig-param">2</em>, <em class="sig-param">1</em>, <em class="sig-param">1)</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/rnn_encoder.html#RNNEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.rnn_encoder.RNNEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>RNNEncoder class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – The number of expected features in the input</p></li>
<li><p><strong>output_size</strong> – The number of output features</p></li>
<li><p><strong>hidden_size</strong> – The number of hidden features</p></li>
<li><p><strong>bidirectional</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code> becomes a bidirectional LSTM</p></li>
<li><p><strong>use_projection</strong> – Use projection layer or not</p></li>
<li><p><strong>num_layers</strong> – Number of recurrent layers</p></li>
<li><p><strong>dropout</strong> – dropout probability</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.rnn_encoder.RNNEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/encoder/rnn_encoder.html#RNNEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.rnn_encoder.RNNEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.rnn_encoder.RNNEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/rnn_encoder.html#RNNEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.rnn_encoder.RNNEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet2-asr-encoder-transformer-encoder">
<span id="id7"></span><h2>espnet2.asr.encoder.transformer_encoder<a class="headerlink" href="#espnet2-asr-encoder-transformer-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.transformer_encoder"></span><p>Encoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.encoder.transformer_encoder.TransformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.transformer_encoder.</code><code class="sig-name descname">TransformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: Optional[str] = 'conv2d'</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">positionwise_layer_type: str = 'linear'</em>, <em class="sig-param">positionwise_conv_kernel_size: int = 1</em>, <em class="sig-param">padding_idx: int = -1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/transformer_encoder.html#TransformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.transformer_encoder.TransformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>Transformer encoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – input dim</p></li>
<li><p><strong>output_size</strong> – dimension of attention</p></li>
<li><p><strong>attention_heads</strong> – the number of heads of multi head attention</p></li>
<li><p><strong>linear_units</strong> – the number of units of position-wise feed forward</p></li>
<li><p><strong>num_blocks</strong> – the number of decoder blocks</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate</p></li>
<li><p><strong>attention_dropout_rate</strong> – dropout rate in attention</p></li>
<li><p><strong>positional_dropout_rate</strong> – dropout rate after adding positional encoding</p></li>
<li><p><strong>input_layer</strong> – input layer type</p></li>
<li><p><strong>pos_enc_class</strong> – PositionalEncoding or ScaledPositionalEncoding</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
<li><p><strong>concat_after</strong> – whether to concat attention layer’s input and output
if True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
if False, no additional linear will be applied.
i.e. x -&gt; x + att(x)</p></li>
<li><p><strong>positionwise_layer_type</strong> – linear of conv1d</p></li>
<li><p><strong>positionwise_conv_kernel_size</strong> – kernel size of positionwise conv1d layer</p></li>
<li><p><strong>padding_idx</strong> – padding_idx for input_layer=embed</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.transformer_encoder.TransformerEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/transformer_encoder.html#TransformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.transformer_encoder.TransformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Embed positions in tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.transformer_encoder.TransformerEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/transformer_encoder.html#TransformerEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.transformer_encoder.TransformerEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet2-asr-encoder-vgg-rnn-encoder">
<span id="id8"></span><h2>espnet2.asr.encoder.vgg_rnn_encoder<a class="headerlink" href="#espnet2-asr-encoder-vgg-rnn-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.vgg_rnn_encoder"></span><dl class="class">
<dt id="espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.vgg_rnn_encoder.</code><code class="sig-name descname">VGGRNNEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">rnn_type: str = 'lstm'</em>, <em class="sig-param">bidirectional: bool = True</em>, <em class="sig-param">use_projection: bool = True</em>, <em class="sig-param">num_layers: int = 4</em>, <em class="sig-param">hidden_size: int = 320</em>, <em class="sig-param">output_size: int = 320</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">in_channel: int = 1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/vgg_rnn_encoder.html#VGGRNNEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>VGGRNNEncoder class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – The number of expected features in the input</p></li>
<li><p><strong>bidirectional</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code> becomes a bidirectional LSTM</p></li>
<li><p><strong>use_projection</strong> – Use projection layer or not</p></li>
<li><p><strong>num_layers</strong> – Number of recurrent layers</p></li>
<li><p><strong>hidden_size</strong> – The number of hidden features</p></li>
<li><p><strong>output_size</strong> – The number of output features</p></li>
<li><p><strong>dropout</strong> – dropout probability</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/encoder/vgg_rnn_encoder.html#VGGRNNEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/vgg_rnn_encoder.html#VGGRNNEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet2-asr-encoder-init">
<span id="id9"></span><h2>espnet2.asr.encoder.__init__<a class="headerlink" href="#espnet2-asr-encoder-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.__init__"></span></div>
<div class="section" id="espnet2-asr-encoder-wav2vec2-encoder">
<span id="id10"></span><h2>espnet2.asr.encoder.wav2vec2_encoder<a class="headerlink" href="#espnet2-asr-encoder-wav2vec2-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.wav2vec2_encoder"></span><p>Encoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.wav2vec2_encoder.</code><code class="sig-name descname">FairSeqWav2Vec2Encoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">w2v_url: str</em>, <em class="sig-param">w2v_dir_path: str = './'</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">freeze_w2v: bool = True</em>, <em class="sig-param">finetune_last_n_layers: int = 0</em>, <em class="sig-param">freeze_finetune_updates: int = 0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/wav2vec2_encoder.html#FairSeqWav2Vec2Encoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>FairSeq Wav2Vec2 encoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – input dim</p></li>
<li><p><strong>output_size</strong> – dimension of attention</p></li>
<li><p><strong>w2v_url</strong> – url to Wav2Vec2.0 pretrained model</p></li>
<li><p><strong>w2v_dir_path</strong> – directory to download the Wav2Vec2.0 pretrained model.</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
<li><p><strong>freeze_w2v</strong> – whether to freeze the Wav2Vec2.0 model during training</p></li>
<li><p><strong>finetune_last_n_layers</strong> – last n layers to be finetuned in Wav2Vec2.0
0 means to finetune every layer if freeze_w2v=False.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/wav2vec2_encoder.html#FairSeqWav2Vec2Encoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward FairSeqWav2Vec2 Encoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/wav2vec2_encoder.html#FairSeqWav2Vec2Encoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder.reload_pretrained_parameters">
<code class="sig-name descname">reload_pretrained_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/wav2vec2_encoder.html#FairSeqWav2Vec2Encoder.reload_pretrained_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder.reload_pretrained_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.asr.encoder.wav2vec2_encoder.download_w2v">
<code class="sig-prename descclassname">espnet2.asr.encoder.wav2vec2_encoder.</code><code class="sig-name descname">download_w2v</code><span class="sig-paren">(</span><em class="sig-param">url</em>, <em class="sig-param">dir_path</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/wav2vec2_encoder.html#download_w2v"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.wav2vec2_encoder.download_w2v" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="espnet2-asr-encoder-extract-features">
<span id="id11"></span><h2>espnet2.asr.encoder.extract_features<a class="headerlink" href="#espnet2-asr-encoder-extract-features" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.extract_features"></span><p>Monkey patch wav2vec2.py Transformer Encoder extract_features().</p>
<dl class="function">
<dt id="espnet2.asr.encoder.extract_features.patched_extract_features">
<code class="sig-prename descclassname">espnet2.asr.encoder.extract_features.</code><code class="sig-name descname">patched_extract_features</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">x</em>, <em class="sig-param">padding_mask=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/extract_features.html#patched_extract_features"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.extract_features.patched_extract_features" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="espnet2-asr-encoder-abs-encoder">
<span id="id12"></span><h2>espnet2.asr.encoder.abs_encoder<a class="headerlink" href="#espnet2-asr-encoder-abs-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.abs_encoder"></span><dl class="class">
<dt id="espnet2.asr.encoder.abs_encoder.AbsEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.abs_encoder.</code><code class="sig-name descname">AbsEncoder</code><a class="reference internal" href="../_modules/espnet2/asr/encoder/abs_encoder.html#AbsEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.encoder.abs_encoder.AbsEncoder.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/abs_encoder.html#AbsEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.abs_encoder.AbsEncoder.output_size">
<em class="property">abstract </em><code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/abs_encoder.html#AbsEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet2-asr-frontend-windowing">
<span id="id13"></span><h2>espnet2.asr.frontend.windowing<a class="headerlink" href="#espnet2-asr-frontend-windowing" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.frontend.windowing"></span><p>Sliding Window for raw audio input data.</p>
<dl class="class">
<dt id="espnet2.asr.frontend.windowing.SlidingWindow">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.frontend.windowing.</code><code class="sig-name descname">SlidingWindow</code><span class="sig-paren">(</span><em class="sig-param">win_length: int = 400</em>, <em class="sig-param">hop_length: int = 160</em>, <em class="sig-param">channels: int = 1</em>, <em class="sig-param">padding: int = None</em>, <em class="sig-param">fs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/frontend/windowing.html#SlidingWindow"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.windowing.SlidingWindow" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend" title="espnet2.asr.frontend.abs_frontend.AbsFrontend"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.frontend.abs_frontend.AbsFrontend</span></code></a></p>
<p>Sliding Window.</p>
<p>Provides a sliding window over a batched continuous raw audio tensor.
Optionally, provides padding (Currently not implemented).
Combine this module with a pre-encoder compatible with raw audio data,
for example Sinc convolutions.</p>
<p>Known issues:
Output length is calculated incorrectly if audio shorter than win_length.
WARNING: trailing values are discarded - padding not implemented yet.
There is currently no additional window function applied to input values.</p>
<p>Initialize.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>win_length</strong> – Length of frame.</p></li>
<li><p><strong>hop_length</strong> – Relative starting point of next frame.</p></li>
<li><p><strong>channels</strong> – Number of input channels.</p></li>
<li><p><strong>padding</strong> – Padding (placeholder, currently not implemented).</p></li>
<li><p><strong>fs</strong> – Sampling rate (placeholder for compatibility, not used).</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.frontend.windowing.SlidingWindow.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/frontend/windowing.html#SlidingWindow.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.windowing.SlidingWindow.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply a sliding window on the input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – Input (B, T, C*D) or (B, T*C*D), with D=C=1.</p></li>
<li><p><strong>input_lengths</strong> – Input lengths within batch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output with dimensions (B, T, C, D), with D=win_length.
Tensor: Output lengths within batch.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.frontend.windowing.SlidingWindow.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/frontend/windowing.html#SlidingWindow.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.windowing.SlidingWindow.output_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Return output length of feature dimension D, i.e. the window length.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet2-asr-frontend-default">
<span id="id14"></span><h2>espnet2.asr.frontend.default<a class="headerlink" href="#espnet2-asr-frontend-default" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.frontend.default"></span><dl class="class">
<dt id="espnet2.asr.frontend.default.DefaultFrontend">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.frontend.default.</code><code class="sig-name descname">DefaultFrontend</code><span class="sig-paren">(</span><em class="sig-param">fs: Union[int</em>, <em class="sig-param">str] = 16000</em>, <em class="sig-param">n_fft: int = 512</em>, <em class="sig-param">win_length: int = None</em>, <em class="sig-param">hop_length: int = 128</em>, <em class="sig-param">window: Optional[str] = 'hann'</em>, <em class="sig-param">center: bool = True</em>, <em class="sig-param">normalized: bool = False</em>, <em class="sig-param">onesided: bool = True</em>, <em class="sig-param">n_mels: int = 80</em>, <em class="sig-param">fmin: int = None</em>, <em class="sig-param">fmax: int = None</em>, <em class="sig-param">htk: bool = False</em>, <em class="sig-param">frontend_conf: Optional[dict] = {'badim': 320</em>, <em class="sig-param">'bdropout_rate': 0.0</em>, <em class="sig-param">'blayers': 3</em>, <em class="sig-param">'bnmask': 2</em>, <em class="sig-param">'bprojs': 320</em>, <em class="sig-param">'btype': 'blstmp'</em>, <em class="sig-param">'bunits': 300</em>, <em class="sig-param">'delay': 3</em>, <em class="sig-param">'ref_channel': -1</em>, <em class="sig-param">'taps': 5</em>, <em class="sig-param">'use_beamformer': False</em>, <em class="sig-param">'use_dnn_mask_for_wpe': True</em>, <em class="sig-param">'use_wpe': False</em>, <em class="sig-param">'wdropout_rate': 0.0</em>, <em class="sig-param">'wlayers': 3</em>, <em class="sig-param">'wprojs': 320</em>, <em class="sig-param">'wtype': 'blstmp'</em>, <em class="sig-param">'wunits': 300}</em>, <em class="sig-param">apply_stft: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/frontend/default.html#DefaultFrontend"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.default.DefaultFrontend" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend" title="espnet2.asr.frontend.abs_frontend.AbsFrontend"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.frontend.abs_frontend.AbsFrontend</span></code></a></p>
<p>Conventional frontend structure for ASR.</p>
<p>Stft -&gt; WPE -&gt; MVDR-Beamformer -&gt; Power-spec -&gt; Mel-Fbank -&gt; CMVN</p>
<dl class="method">
<dt id="espnet2.asr.frontend.default.DefaultFrontend.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/frontend/default.html#DefaultFrontend.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.default.DefaultFrontend.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.frontend.default.DefaultFrontend.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/frontend/default.html#DefaultFrontend.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.default.DefaultFrontend.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet2-asr-frontend-init">
<span id="id15"></span><h2>espnet2.asr.frontend.__init__<a class="headerlink" href="#espnet2-asr-frontend-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.frontend.__init__"></span></div>
<div class="section" id="espnet2-asr-frontend-abs-frontend">
<span id="id16"></span><h2>espnet2.asr.frontend.abs_frontend<a class="headerlink" href="#espnet2-asr-frontend-abs-frontend" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.frontend.abs_frontend"></span><dl class="class">
<dt id="espnet2.asr.frontend.abs_frontend.AbsFrontend">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.frontend.abs_frontend.</code><code class="sig-name descname">AbsFrontend</code><a class="reference internal" href="../_modules/espnet2/asr/frontend/abs_frontend.html#AbsFrontend"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.frontend.abs_frontend.AbsFrontend.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/frontend/abs_frontend.html#AbsFrontend.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.frontend.abs_frontend.AbsFrontend.output_size">
<em class="property">abstract </em><code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/frontend/abs_frontend.html#AbsFrontend.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet2-asr-decoder-rnn-decoder">
<span id="id17"></span><h2>espnet2.asr.decoder.rnn_decoder<a class="headerlink" href="#espnet2-asr-decoder-rnn-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.decoder.rnn_decoder"></span><dl class="class">
<dt id="espnet2.asr.decoder.rnn_decoder.RNNDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.rnn_decoder.</code><code class="sig-name descname">RNNDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">rnn_type: str = 'lstm'</em>, <em class="sig-param">num_layers: int = 1</em>, <em class="sig-param">hidden_size: int = 320</em>, <em class="sig-param">sampling_probability: float = 0.0</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">context_residual: bool = False</em>, <em class="sig-param">replace_sos: bool = False</em>, <em class="sig-param">num_encs: int = 1</em>, <em class="sig-param">att_conf: dict = {'aconv_chans': 10</em>, <em class="sig-param">'aconv_filts': 100</em>, <em class="sig-param">'adim': 320</em>, <em class="sig-param">'aheads': 4</em>, <em class="sig-param">'atype': 'location'</em>, <em class="sig-param">'awin': 5</em>, <em class="sig-param">'han_conv_chans': -1</em>, <em class="sig-param">'han_conv_filts': 100</em>, <em class="sig-param">'han_dim': 320</em>, <em class="sig-param">'han_heads': 4</em>, <em class="sig-param">'han_mode': False</em>, <em class="sig-param">'han_type': None</em>, <em class="sig-param">'han_win': 5</em>, <em class="sig-param">'num_att': 1</em>, <em class="sig-param">'num_encs': 1}</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#RNNDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.RNNDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder" title="espnet2.asr.decoder.abs_decoder.AbsDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.abs_decoder.AbsDecoder</span></code></a></p>
<dl class="method">
<dt id="espnet2.asr.decoder.rnn_decoder.RNNDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em>, <em class="sig-param">hlens</em>, <em class="sig-param">ys_in_pad</em>, <em class="sig-param">ys_in_lens</em>, <em class="sig-param">strm_idx=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#RNNDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.RNNDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.rnn_decoder.RNNDecoder.init_state">
<code class="sig-name descname">init_state</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#RNNDecoder.init_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.RNNDecoder.init_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Get an initial state for decoding (optional).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – The encoded feature tensor</p>
</dd>
</dl>
<p>Returns: initial state</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.rnn_decoder.RNNDecoder.rnn_forward">
<code class="sig-name descname">rnn_forward</code><span class="sig-paren">(</span><em class="sig-param">ey</em>, <em class="sig-param">z_list</em>, <em class="sig-param">c_list</em>, <em class="sig-param">z_prev</em>, <em class="sig-param">c_prev</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#RNNDecoder.rnn_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.RNNDecoder.rnn_forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.rnn_decoder.RNNDecoder.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">yseq</em>, <em class="sig-param">state</em>, <em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#RNNDecoder.score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.RNNDecoder.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score new token (required).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y</strong> (<em>torch.Tensor</em>) – 1D torch.int64 prefix tokens.</p></li>
<li><p><strong>state</strong> – Scorer state for prefix tokens</p></li>
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – The encoder feature that generates ys.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Tuple of</dt><dd><p>scores for next token that has a shape of <cite>(n_vocab)</cite>
and next state for ys</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple[torch.Tensor, Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.rnn_decoder.RNNDecoder.zero_state">
<code class="sig-name descname">zero_state</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#RNNDecoder.zero_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.RNNDecoder.zero_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.asr.decoder.rnn_decoder.build_attention_list">
<code class="sig-prename descclassname">espnet2.asr.decoder.rnn_decoder.</code><code class="sig-name descname">build_attention_list</code><span class="sig-paren">(</span><em class="sig-param">eprojs: int</em>, <em class="sig-param">dunits: int</em>, <em class="sig-param">atype: str = 'location'</em>, <em class="sig-param">num_att: int = 1</em>, <em class="sig-param">num_encs: int = 1</em>, <em class="sig-param">aheads: int = 4</em>, <em class="sig-param">adim: int = 320</em>, <em class="sig-param">awin: int = 5</em>, <em class="sig-param">aconv_chans: int = 10</em>, <em class="sig-param">aconv_filts: int = 100</em>, <em class="sig-param">han_mode: bool = False</em>, <em class="sig-param">han_type=None</em>, <em class="sig-param">han_heads: int = 4</em>, <em class="sig-param">han_dim: int = 320</em>, <em class="sig-param">han_conv_chans: int = -1</em>, <em class="sig-param">han_conv_filts: int = 100</em>, <em class="sig-param">han_win: int = 5</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#build_attention_list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.build_attention_list" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="espnet2-asr-decoder-abs-decoder">
<span id="id18"></span><h2>espnet2.asr.decoder.abs_decoder<a class="headerlink" href="#espnet2-asr-decoder-abs-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.decoder.abs_decoder"></span><dl class="class">
<dt id="espnet2.asr.decoder.abs_decoder.AbsDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.abs_decoder.</code><code class="sig-name descname">AbsDecoder</code><a class="reference internal" href="../_modules/espnet2/asr/decoder/abs_decoder.html#AbsDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <a class="reference internal" href="espnet.nets.html#espnet.nets.scorer_interface.ScorerInterface" title="espnet.nets.scorer_interface.ScorerInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.nets.scorer_interface.ScorerInterface</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.decoder.abs_decoder.AbsDecoder.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad: torch.Tensor</em>, <em class="sig-param">hlens: torch.Tensor</em>, <em class="sig-param">ys_in_pad: torch.Tensor</em>, <em class="sig-param">ys_in_lens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/decoder/abs_decoder.html#AbsDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet2-asr-decoder-transformer-decoder">
<span id="id19"></span><h2>espnet2.asr.decoder.transformer_decoder<a class="headerlink" href="#espnet2-asr-decoder-transformer-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.decoder.transformer_decoder"></span><p>Decoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transformer_decoder.</code><code class="sig-name descname">BaseTransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#BaseTransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder" title="espnet2.asr.decoder.abs_decoder.AbsDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.abs_decoder.AbsDecoder</span></code></a>, <a class="reference internal" href="espnet.nets.html#espnet.nets.scorer_interface.BatchScorerInterface" title="espnet.nets.scorer_interface.BatchScorerInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.nets.scorer_interface.BatchScorerInterface</span></code></a></p>
<p>Base class of Transfomer decoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> – output dim</p></li>
<li><p><strong>encoder_output_size</strong> – dimension of attention</p></li>
<li><p><strong>attention_heads</strong> – the number of heads of multi head attention</p></li>
<li><p><strong>linear_units</strong> – the number of units of position-wise feed forward</p></li>
<li><p><strong>num_blocks</strong> – the number of decoder blocks</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate</p></li>
<li><p><strong>self_attention_dropout_rate</strong> – dropout rate for attention</p></li>
<li><p><strong>input_layer</strong> – input layer type</p></li>
<li><p><strong>use_output_layer</strong> – whether to use output layer</p></li>
<li><p><strong>pos_enc_class</strong> – PositionalEncoding or ScaledPositionalEncoding</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
<li><p><strong>concat_after</strong> – whether to concat attention layer’s input and output
if True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
if False, no additional linear will be applied.
i.e. x -&gt; x + att(x)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.batch_score">
<code class="sig-name descname">batch_score</code><span class="sig-paren">(</span><em class="sig-param">ys: torch.Tensor, states: List[Any], xs: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, List[Any]]<a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#BaseTransformerDecoder.batch_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.batch_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score new token batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ys</strong> (<em>torch.Tensor</em>) – torch.int64 prefix tokens (n_batch, ylen).</p></li>
<li><p><strong>states</strong> (<em>List</em><em>[</em><em>Any</em><em>]</em>) – Scorer states for prefix tokens.</p></li>
<li><p><strong>xs</strong> (<em>torch.Tensor</em>) – The encoder feature that generates ys (n_batch, xlen, n_feat).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Tuple of</dt><dd><p>batchfied scores for next token with shape of <cite>(n_batch, n_vocab)</cite>
and next state list for ys.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple[torch.Tensor, List[Any]]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad: torch.Tensor</em>, <em class="sig-param">hlens: torch.Tensor</em>, <em class="sig-param">ys_in_pad: torch.Tensor</em>, <em class="sig-param">ys_in_lens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#BaseTransformerDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward decoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hs_pad</strong> – encoded memory, float32  (batch, maxlen_in, feat)</p></li>
<li><p><strong>hlens</strong> – (batch)</p></li>
<li><p><strong>ys_in_pad</strong> – input token ids, int64 (batch, maxlen_out)
if input_layer == “embed”
input tensor (batch, maxlen_out, #mels) in the other cases</p></li>
<li><p><strong>ys_in_lens</strong> – (batch)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>tuple containing:</p>
<dl class="simple">
<dt>x: decoded token score before softmax (batch, maxlen_out, token)</dt><dd><p>if use_output_layer is True,</p>
</dd>
</dl>
<p>olens: (batch, )</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(tuple)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.forward_one_step">
<code class="sig-name descname">forward_one_step</code><span class="sig-paren">(</span><em class="sig-param">tgt: torch.Tensor</em>, <em class="sig-param">tgt_mask: torch.Tensor</em>, <em class="sig-param">memory: torch.Tensor</em>, <em class="sig-param">cache: List[torch.Tensor] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, List[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#BaseTransformerDecoder.forward_one_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.forward_one_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward one step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tgt</strong> – input token ids, int64 (batch, maxlen_out)</p></li>
<li><p><strong>tgt_mask</strong> – input token mask,  (batch, maxlen_out)
dtype=torch.uint8 in PyTorch 1.2-
dtype=torch.bool in PyTorch 1.2+ (include 1.2)</p></li>
<li><p><strong>memory</strong> – encoded memory, float32  (batch, maxlen_in, feat)</p></li>
<li><p><strong>cache</strong> – cached output list of (batch, max_time_out-1, size)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>NN output value and cache per <cite>self.decoders</cite>.
y.shape` is (batch, maxlen_out, token)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>y, cache</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">ys</em>, <em class="sig-param">state</em>, <em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#BaseTransformerDecoder.score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.decoder.transformer_decoder.DynamicConvolution2DTransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transformer_decoder.</code><code class="sig-name descname">DynamicConvolution2DTransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">self_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">src_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">conv_wshare: int = 4</em>, <em class="sig-param">conv_kernel_length: Sequence[int] = (11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11)</em>, <em class="sig-param">conv_usebias: int = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#DynamicConvolution2DTransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.DynamicConvolution2DTransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder" title="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="espnet2.asr.decoder.transformer_decoder.DynamicConvolutionTransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transformer_decoder.</code><code class="sig-name descname">DynamicConvolutionTransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">self_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">src_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">conv_wshare: int = 4</em>, <em class="sig-param">conv_kernel_length: Sequence[int] = (11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11)</em>, <em class="sig-param">conv_usebias: int = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#DynamicConvolutionTransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.DynamicConvolutionTransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder" title="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="espnet2.asr.decoder.transformer_decoder.LightweightConvolution2DTransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transformer_decoder.</code><code class="sig-name descname">LightweightConvolution2DTransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">self_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">src_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">conv_wshare: int = 4</em>, <em class="sig-param">conv_kernel_length: Sequence[int] = (11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11)</em>, <em class="sig-param">conv_usebias: int = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#LightweightConvolution2DTransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.LightweightConvolution2DTransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder" title="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="espnet2.asr.decoder.transformer_decoder.LightweightConvolutionTransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transformer_decoder.</code><code class="sig-name descname">LightweightConvolutionTransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">self_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">src_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">conv_wshare: int = 4</em>, <em class="sig-param">conv_kernel_length: Sequence[int] = (11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11)</em>, <em class="sig-param">conv_usebias: int = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#LightweightConvolutionTransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.LightweightConvolutionTransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder" title="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="espnet2.asr.decoder.transformer_decoder.TransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transformer_decoder.</code><code class="sig-name descname">TransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">self_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">src_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#TransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.TransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder" title="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder</span></code></a></p>
</dd></dl>

</div>
<div class="section" id="espnet2-asr-decoder-init">
<span id="id20"></span><h2>espnet2.asr.decoder.__init__<a class="headerlink" href="#espnet2-asr-decoder-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.decoder.__init__"></span></div>
<div class="section" id="espnet2-asr-specaug-specaug">
<span id="id21"></span><h2>espnet2.asr.specaug.specaug<a class="headerlink" href="#espnet2-asr-specaug-specaug" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.specaug.specaug"></span><dl class="class">
<dt id="espnet2.asr.specaug.specaug.SpecAug">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.specaug.specaug.</code><code class="sig-name descname">SpecAug</code><span class="sig-paren">(</span><em class="sig-param">apply_time_warp: bool = True</em>, <em class="sig-param">time_warp_window: int = 5</em>, <em class="sig-param">time_warp_mode: str = 'bicubic'</em>, <em class="sig-param">apply_freq_mask: bool = True</em>, <em class="sig-param">freq_mask_width_range: Union[int</em>, <em class="sig-param">Sequence[int]] = (0</em>, <em class="sig-param">20)</em>, <em class="sig-param">num_freq_mask: int = 2</em>, <em class="sig-param">apply_time_mask: bool = True</em>, <em class="sig-param">time_mask_width_range: Union[int</em>, <em class="sig-param">Sequence[int]] = (0</em>, <em class="sig-param">100)</em>, <em class="sig-param">num_time_mask: int = 2</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/specaug/specaug.html#SpecAug"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.specaug.specaug.SpecAug" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.specaug.abs_specaug.AbsSpecAug" title="espnet2.asr.specaug.abs_specaug.AbsSpecAug"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.specaug.abs_specaug.AbsSpecAug</span></code></a></p>
<p>Implementation of SpecAug.</p>
<dl>
<dt>Reference:</dt><dd><p>Daniel S. Park et al.
“SpecAugment: A Simple Data</p>
<blockquote>
<div><p>Augmentation Method for Automatic Speech Recognition”</p>
</div></blockquote>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When using cuda mode, time_warp doesn’t have reproducibility
due to <cite>torch.nn.functional.interpolate</cite>.</p>
</div>
<dl class="method">
<dt id="espnet2.asr.specaug.specaug.SpecAug.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">x_lengths=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/specaug/specaug.html#SpecAug.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.specaug.specaug.SpecAug.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet2-asr-specaug-abs-specaug">
<span id="id22"></span><h2>espnet2.asr.specaug.abs_specaug<a class="headerlink" href="#espnet2-asr-specaug-abs-specaug" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.specaug.abs_specaug"></span><dl class="class">
<dt id="espnet2.asr.specaug.abs_specaug.AbsSpecAug">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.specaug.abs_specaug.</code><code class="sig-name descname">AbsSpecAug</code><a class="reference internal" href="../_modules/espnet2/asr/specaug/abs_specaug.html#AbsSpecAug"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.specaug.abs_specaug.AbsSpecAug" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Abstract class for the augmentation of spectrogram</p>
<p>The process-flow:</p>
<p>Frontend  -&gt; SpecAug -&gt; Normalization -&gt; Encoder -&gt; Decoder</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.specaug.abs_specaug.AbsSpecAug.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x: torch.Tensor</em>, <em class="sig-param">x_lengths: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/specaug/abs_specaug.html#AbsSpecAug.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.specaug.abs_specaug.AbsSpecAug.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet2-asr-specaug-init">
<span id="id23"></span><h2>espnet2.asr.specaug.__init__<a class="headerlink" href="#espnet2-asr-specaug-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.specaug.__init__"></span></div>
<div class="section" id="espnet2-asr-preencoder-sinc">
<span id="id24"></span><h2>espnet2.asr.preencoder.sinc<a class="headerlink" href="#espnet2-asr-preencoder-sinc" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.preencoder.sinc"></span><p>Sinc convolutions for raw audio input.</p>
<dl class="class">
<dt id="espnet2.asr.preencoder.sinc.LightweightSincConvs">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.preencoder.sinc.</code><code class="sig-name descname">LightweightSincConvs</code><span class="sig-paren">(</span><em class="sig-param">fs: Union[int</em>, <em class="sig-param">str</em>, <em class="sig-param">float] = 16000</em>, <em class="sig-param">in_channels: int = 1</em>, <em class="sig-param">out_channels: int = 256</em>, <em class="sig-param">activation_type: str = 'leakyrelu'</em>, <em class="sig-param">dropout_type: str = 'dropout'</em>, <em class="sig-param">windowing_type: str = 'hamming'</em>, <em class="sig-param">scale_type: str = 'mel'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#LightweightSincConvs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.LightweightSincConvs" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder" title="espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder</span></code></a></p>
<p>Lightweight Sinc Convolutions.</p>
<p>Instead of using precomputed features, end-to-end speech recognition
can also be done directly from raw audio using sinc convolutions, as
described in “Lightweight End-to-End Speech Recognition from Raw Audio
Data Using Sinc-Convolutions” by Kürzinger et al.
<a class="reference external" href="https://arxiv.org/abs/2010.07597">https://arxiv.org/abs/2010.07597</a></p>
<p>To use Sinc convolutions in your model instead of the default f-bank
frontend, set this module as your pre-encoder with <cite>preencoder: sinc</cite>
and use the input of the sliding window frontend with
<cite>frontend: sliding_window</cite> in your yaml configuration file.
So that the process flow is:</p>
<p>Frontend (SlidingWindow) -&gt; SpecAug -&gt; Normalization -&gt;
Pre-encoder (LightweightSincConvs) -&gt; Encoder -&gt; Decoder</p>
<p>Note that this method also performs data augmentation in time domain
(vs. in spectral domain in the default frontend).
Use <cite>plot_sinc_filters.py</cite> to visualize the learned Sinc filters.</p>
<p>Initialize the module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fs</strong> – Sample rate.</p></li>
<li><p><strong>in_channels</strong> – Number of input channels.</p></li>
<li><p><strong>out_channels</strong> – Number of output channels (for each input channel).</p></li>
<li><p><strong>activation_type</strong> – Choice of activation function.</p></li>
<li><p><strong>dropout_type</strong> – Choice of dropout function.</p></li>
<li><p><strong>windowing_type</strong> – Choice of windowing function.</p></li>
<li><p><strong>scale_type</strong> – Choice of filter-bank initialization scale.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.preencoder.sinc.LightweightSincConvs.espnet_initialization_fn">
<code class="sig-name descname">espnet_initialization_fn</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#LightweightSincConvs.espnet_initialization_fn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.LightweightSincConvs.espnet_initialization_fn" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize sinc filters with filterbank values.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.preencoder.sinc.LightweightSincConvs.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#LightweightSincConvs.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.LightweightSincConvs.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply Lightweight Sinc Convolutions.</p>
<p>The input shall be formatted as (B, T, C_in, D_in)
with B as batch size, T as time dimension, C_in as channels,
and D_in as feature dimension.</p>
<p>The output will then be (B, T, C_out*D_out)
with C_out and D_out as output dimensions.</p>
<p>The current module structure only handles D_in=400, so that D_out=1.
Remark for the multichannel case: C_out is the number of out_channels
given at initialization multiplied with C_in.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.preencoder.sinc.LightweightSincConvs.gen_lsc_block">
<code class="sig-name descname">gen_lsc_block</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int</em>, <em class="sig-param">out_channels: int</em>, <em class="sig-param">depthwise_kernel_size: int = 9</em>, <em class="sig-param">depthwise_stride: int = 1</em>, <em class="sig-param">depthwise_groups=None</em>, <em class="sig-param">pointwise_groups=0</em>, <em class="sig-param">dropout_probability: float = 0.15</em>, <em class="sig-param">avgpool=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#LightweightSincConvs.gen_lsc_block"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.LightweightSincConvs.gen_lsc_block" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a convolutional block for Lightweight Sinc convolutions.</p>
<p>Each block consists of either a depthwise or a depthwise-separable
convolutions together with dropout, (batch-)normalization layer, and
an optional average-pooling layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> – Number of input channels.</p></li>
<li><p><strong>out_channels</strong> – Number of output channels.</p></li>
<li><p><strong>depthwise_kernel_size</strong> – Kernel size of the depthwise convolution.</p></li>
<li><p><strong>depthwise_stride</strong> – Stride of the depthwise convolution.</p></li>
<li><p><strong>depthwise_groups</strong> – Number of groups of the depthwise convolution.</p></li>
<li><p><strong>pointwise_groups</strong> – Number of groups of the pointwise convolution.</p></li>
<li><p><strong>dropout_probability</strong> – Dropout probability in the block.</p></li>
<li><p><strong>avgpool</strong> – If True, an AvgPool layer is inserted.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Neural network building block.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Sequential</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.preencoder.sinc.LightweightSincConvs.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#LightweightSincConvs.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.LightweightSincConvs.output_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the output size.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.preencoder.sinc.SpatialDropout">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.preencoder.sinc.</code><code class="sig-name descname">SpatialDropout</code><span class="sig-paren">(</span><em class="sig-param">dropout_probability: float = 0.15</em>, <em class="sig-param">shape: Union[tuple</em>, <em class="sig-param">list</em>, <em class="sig-param">None] = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#SpatialDropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.SpatialDropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Spatial dropout module.</p>
<p>Apply dropout to full channels on tensors of input (B, C, D)</p>
<p>Initialize.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dropout_probability</strong> – Dropout probability.</p></li>
<li><p><strong>shape</strong> (<em>tuple</em><em>, </em><em>list</em>) – Shape of input tensors.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.preencoder.sinc.SpatialDropout.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#SpatialDropout.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.SpatialDropout.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward of spatial dropout module.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet2-asr-preencoder-abs-preencoder">
<span id="id25"></span><h2>espnet2.asr.preencoder.abs_preencoder<a class="headerlink" href="#espnet2-asr-preencoder-abs-preencoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.preencoder.abs_preencoder"></span><dl class="class">
<dt id="espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.preencoder.abs_preencoder.</code><code class="sig-name descname">AbsPreEncoder</code><a class="reference internal" href="../_modules/espnet2/asr/preencoder/abs_preencoder.html#AbsPreEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/preencoder/abs_preencoder.html#AbsPreEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder.output_size">
<em class="property">abstract </em><code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/preencoder/abs_preencoder.html#AbsPreEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet2-asr-preencoder-init">
<span id="id26"></span><h2>espnet2.asr.preencoder.__init__<a class="headerlink" href="#espnet2-asr-preencoder-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.preencoder.__init__"></span></div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="espnet2.fileio.html" class="btn btn-neutral float-right" title="espnet2.fileio package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="espnet2.main_funcs.html" class="btn btn-neutral float-left" title="espnet2.main_funcs package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2017, Shinji Watanabe.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>